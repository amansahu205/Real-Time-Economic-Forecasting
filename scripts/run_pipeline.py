#!/usr/bin/env python3
"""
End-to-End Economic Forecasting Pipeline

This script runs the complete pipeline:
1. Satellite Detection (if needed)
2. AIS Data Processing (if needed)
3. Feature Extraction
4. Feature Fusion
5. Model Training
6. Prediction Generation

Usage:
    python scripts/run_pipeline.py --all
    python scripts/run_pipeline.py --features-only
    python scripts/run_pipeline.py --train-only
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime
import logging

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.config import (
    ANNOTATIONS_DIR, FEATURES_DIR, MODELS_DIR, 
    AIS_PROCESSED_DIR, LOCATIONS
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def check_prerequisites():
    """Check if required data exists."""
    status = {
        'satellite_detections': False,
        'ais_data': False,
        'models': False
    }
    
    # Check satellite detections
    port_la_summary = ANNOTATIONS_DIR / "google_earth_tiled" / "Port_of_LA" / "all_years_summary.csv"
    if port_la_summary.exists():
        status['satellite_detections'] = True
        logger.info("‚úÖ Satellite detections found")
    else:
        logger.warning("‚ùå Satellite detections not found")
    
    # Check AIS data
    ais_daily = AIS_PROCESSED_DIR / "Port_of_LA_ais_daily.csv"
    if ais_daily.exists():
        status['ais_data'] = True
        logger.info("‚úÖ AIS data found")
    else:
        logger.warning("‚ùå AIS data not found - run: python scripts/process_ais_data.py")
    
    # Check models
    from src.config import PORTS_MODEL
    if PORTS_MODEL.exists():
        status['models'] = True
        logger.info("‚úÖ YOLO models found")
    else:
        logger.warning("‚ùå YOLO models not found")
    
    return status


def run_satellite_detection():
    """Run satellite detection pipeline."""
    logger.info("\n" + "="*60)
    logger.info("üõ∞Ô∏è STEP 1: SATELLITE DETECTION")
    logger.info("="*60)
    
    from src.detection.tiled_detector import TiledDetector
    from src.detection.annotation_manager import AnnotationManager
    from src.config import GOOGLE_EARTH_DIR, PORTS_MODEL
    
    detector = TiledDetector(str(PORTS_MODEL))
    
    for location in LOCATIONS['ports']:
        location_dir = GOOGLE_EARTH_DIR / location
        if not location_dir.exists():
            logger.warning(f"Location not found: {location}")
            continue
        
        logger.info(f"Processing {location}...")
        # Detection logic here (already implemented in existing scripts)
    
    logger.info("‚úÖ Satellite detection complete")


def run_ais_processing():
    """Run AIS data processing."""
    logger.info("\n" + "="*60)
    logger.info("üö¢ STEP 2: AIS DATA PROCESSING")
    logger.info("="*60)
    
    import subprocess
    result = subprocess.run(
        [sys.executable, "scripts/process_ais_data.py", "--year", "2017"],
        cwd=PROJECT_ROOT,
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        logger.info("‚úÖ AIS processing complete")
    else:
        logger.error(f"AIS processing failed: {result.stderr}")


def run_feature_extraction():
    """Run feature extraction from all sources."""
    logger.info("\n" + "="*60)
    logger.info("üìä STEP 3: FEATURE EXTRACTION")
    logger.info("="*60)
    
    # Ensure features directory exists
    FEATURES_DIR.mkdir(parents=True, exist_ok=True)
    
    # Extract satellite features
    logger.info("Extracting satellite features...")
    from src.features.satellite_features import SatelliteFeatureExtractor
    
    sat_extractor = SatelliteFeatureExtractor(ANNOTATIONS_DIR)
    sat_features = sat_extractor.process_all_locations(
        LOCATIONS['ports'],
        dataset="google_earth_tiled"
    )
    
    if not sat_features.empty:
        sat_extractor.save_features(sat_features, FEATURES_DIR / "satellite_port_features.csv")
        logger.info(f"  ‚úÖ Satellite features: {len(sat_features)} records")
    else:
        logger.warning("  ‚ö†Ô∏è No satellite features extracted")
    
    # Extract AIS features (if available)
    logger.info("Extracting AIS features...")
    from src.features.ais_features import AISFeatureExtractor
    
    ais_extractor = AISFeatureExtractor(AIS_PROCESSED_DIR)
    ais_daily = ais_extractor.load_daily_metrics("Port_of_LA")
    
    if not ais_daily.empty:
        ais_features = ais_extractor.extract_features(ais_daily)
        ais_extractor.save_features(ais_features, FEATURES_DIR / "ais_daily_features.csv")
        
        ais_yearly = ais_extractor.aggregate_yearly(ais_features)
        ais_extractor.save_features(ais_yearly, FEATURES_DIR / "ais_yearly_features.csv")
        
        logger.info(f"  ‚úÖ AIS daily features: {len(ais_features)} records")
        logger.info(f"  ‚úÖ AIS yearly features: {len(ais_yearly)} records")
    else:
        logger.warning("  ‚ö†Ô∏è No AIS features extracted")
    
    logger.info("‚úÖ Feature extraction complete")


def run_feature_fusion():
    """Run feature fusion."""
    logger.info("\n" + "="*60)
    logger.info("üîó STEP 4: FEATURE FUSION")
    logger.info("="*60)
    
    from src.features.feature_fusion import FeatureFusion
    
    fusion = FeatureFusion(FEATURES_DIR)
    
    # Fuse yearly features
    yearly = fusion.fuse_yearly_features()
    if not yearly.empty:
        fusion.save_unified_features(yearly, "yearly_features")
        logger.info(f"  ‚úÖ Unified yearly features: {len(yearly)} records")
    
    # Fuse daily features
    daily = fusion.fuse_daily_features()
    if not daily.empty:
        fusion.save_unified_features(daily, "daily_features")
        logger.info(f"  ‚úÖ Unified daily features: {len(daily)} records")
    
    # Create forecasting dataset
    forecast_df = fusion.create_forecasting_dataset()
    if not forecast_df.empty:
        fusion.save_unified_features(forecast_df, "forecasting_dataset")
        logger.info(f"  ‚úÖ Forecasting dataset: {len(forecast_df)} records")
    
    logger.info("‚úÖ Feature fusion complete")


def run_model_training():
    """Run model training."""
    logger.info("\n" + "="*60)
    logger.info("ü§ñ STEP 5: MODEL TRAINING")
    logger.info("="*60)
    
    from src.forecasting.model import train_forecasting_model
    
    features_path = FEATURES_DIR / "unified_forecasting_dataset.csv"
    
    if not features_path.exists():
        logger.error(f"Forecasting dataset not found: {features_path}")
        return None
    
    # Train model
    forecaster = train_forecasting_model(
        features_path,
        target_col='port_activity_index',
        model_type='random_forest'
    )
    
    # Save model
    model_dir = MODELS_DIR / "forecasting"
    model_dir.mkdir(parents=True, exist_ok=True)
    
    model_path = model_dir / f"economic_forecaster_{datetime.now().strftime('%Y%m%d')}.pkl"
    forecaster.save_model(model_path)
    
    logger.info(f"‚úÖ Model trained and saved: {model_path}")
    logger.info(f"   R¬≤ Score: {forecaster.metrics['r2']:.4f}")
    logger.info(f"   MAPE: {forecaster.metrics['mape']:.2f}%")
    
    return forecaster


def run_full_pipeline():
    """Run the complete pipeline."""
    start_time = datetime.now()
    
    logger.info("="*60)
    logger.info("üöÄ ECONOMIC FORECASTING PIPELINE")
    logger.info("="*60)
    logger.info(f"Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Check prerequisites
    status = check_prerequisites()
    
    # Run pipeline steps
    if not status['satellite_detections']:
        logger.info("Skipping satellite detection (already done or no images)")
    
    if not status['ais_data']:
        run_ais_processing()
    
    run_feature_extraction()
    run_feature_fusion()
    
    # Check if we have enough data for training
    forecast_path = FEATURES_DIR / "unified_forecasting_dataset.csv"
    if forecast_path.exists():
        run_model_training()
    else:
        logger.warning("Not enough data for model training")
    
    # Summary
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    logger.info("\n" + "="*60)
    logger.info("‚úÖ PIPELINE COMPLETE")
    logger.info("="*60)
    logger.info(f"Duration: {duration:.1f} seconds")
    logger.info(f"Features saved to: {FEATURES_DIR}")
    logger.info(f"Models saved to: {MODELS_DIR}")


def main():
    parser = argparse.ArgumentParser(description='Economic Forecasting Pipeline')
    parser.add_argument('--all', action='store_true', help='Run full pipeline')
    parser.add_argument('--features-only', action='store_true', help='Run feature extraction only')
    parser.add_argument('--train-only', action='store_true', help='Run model training only')
    parser.add_argument('--check', action='store_true', help='Check prerequisites only')
    
    args = parser.parse_args()
    
    if args.check:
        check_prerequisites()
    elif args.features_only:
        run_feature_extraction()
        run_feature_fusion()
    elif args.train_only:
        run_model_training()
    elif args.all:
        run_full_pipeline()
    else:
        # Default: run full pipeline
        run_full_pipeline()


if __name__ == "__main__":
    main()
