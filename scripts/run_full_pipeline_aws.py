#!/usr/bin/env python3
"""
Full End-to-End Economic Forecasting Pipeline for AWS SageMaker

This script runs the complete pipeline:
1. Download satellite images from S3
2. Run YOLO detection on all images
3. Process AIS data
4. Fuse satellite + AIS features
5. Train forecasting models
6. Generate predictions
7. Save annotated images + results to S3

Run in SageMaker notebook or terminal:
    python scripts/run_full_pipeline_aws.py
"""

import os
import sys
import json
import boto3
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Add project root to path
PROJECT_ROOT = Path('/home/ec2-user/SageMaker/Real-Time-Economic-Forecasting')
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'src'))

# AWS Configuration
S3_RAW = 'economic-forecast-raw'
S3_PROCESSED = 'economic-forecast-processed'
S3_MODELS = 'economic-forecast-models'

s3 = boto3.client('s3')

# Pipeline results
PIPELINE_RESULTS = {
    'pipeline_id': datetime.now().strftime('%Y%m%d_%H%M%S'),
    'started_at': datetime.now().isoformat(),
    'steps': {}
}


def print_header(text):
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60)


def print_step(step_num, text):
    print(f"\n{'‚îÄ' * 60}")
    print(f"  STEP {step_num}: {text}")
    print(f"{'‚îÄ' * 60}")


# ============================================================
# STEP 1: Download Data from S3
# ============================================================
def step1_download_data():
    """Download satellite images and models from S3."""
    
    print_step(1, "DOWNLOADING DATA FROM S3")
    
    local_data_dir = PROJECT_ROOT / 'data'
    local_satellite_dir = local_data_dir / 'raw' / 'satellite' / 'google_earth'
    local_models_dir = local_data_dir / 'models' / 'satellite'
    
    # Create directories
    local_satellite_dir.mkdir(parents=True, exist_ok=True)
    local_models_dir.mkdir(parents=True, exist_ok=True)
    
    # Download satellite images
    print("\nüì• Downloading satellite images...")
    
    response = s3.list_objects_v2(Bucket=S3_RAW, Prefix='satellite/google_earth/')
    images_downloaded = 0
    
    for obj in response.get('Contents', []):
        key = obj['Key']
        if key.endswith(('.jpg', '.png')):
            # Create local path
            relative_path = key.replace('satellite/google_earth/', '')
            local_path = local_satellite_dir / relative_path
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            if not local_path.exists():
                s3.download_file(S3_RAW, key, str(local_path))
                images_downloaded += 1
                print(f"   ‚úì {relative_path}")
    
    print(f"\n   ‚úÖ Downloaded {images_downloaded} new images")
    
    # Download YOLO models
    print("\nüì• Downloading YOLO models...")
    
    models = [
        ('yolo/ports/best.pt', local_models_dir / 'ports_best.pt'),
        ('yolo/retail/best.pt', local_models_dir / 'retail_best.pt')
    ]
    
    for s3_key, local_path in models:
        try:
            if not local_path.exists():
                s3.download_file(S3_MODELS, s3_key, str(local_path))
                print(f"   ‚úì {s3_key}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è Could not download {s3_key}: {e}")
    
    PIPELINE_RESULTS['steps']['download'] = {
        'images_downloaded': images_downloaded,
        'status': 'completed'
    }
    
    return local_satellite_dir, local_models_dir


# ============================================================
# STEP 2: Run YOLO Detection
# ============================================================
def step2_run_detection(satellite_dir, models_dir):
    """Run YOLO detection on all satellite images."""
    
    print_step(2, "RUNNING YOLO OBJECT DETECTION")
    
    # Import detection modules
    try:
        from ultralytics import YOLO
        import cv2
    except ImportError:
        print("   ‚ö†Ô∏è Installing ultralytics...")
        os.system('pip install ultralytics opencv-python-headless -q')
        from ultralytics import YOLO
        import cv2
    
    # Load models
    port_model_path = models_dir / 'ports_best.pt'
    retail_model_path = models_dir / 'retail_best.pt'
    
    # Use pretrained if custom not available
    if not port_model_path.exists():
        print("   ‚ö†Ô∏è Using pretrained YOLO model")
        port_model = YOLO('yolov8n.pt')
        retail_model = YOLO('yolov8n.pt')
    else:
        port_model = YOLO(str(port_model_path))
        retail_model = YOLO(str(retail_model_path))
    
    # Results storage
    detection_results = {
        'Port_of_LA': {},
        'Mall_of_america': {}
    }
    
    # Output directories
    annotations_dir = PROJECT_ROOT / 'results' / 'annotations'
    annotations_dir.mkdir(parents=True, exist_ok=True)
    
    # Process Port of LA images
    print("\nüö¢ Processing Port of LA images...")
    port_dir = satellite_dir / 'Port_of_LA'
    
    if port_dir.exists():
        for year_dir in sorted(port_dir.iterdir()):
            if year_dir.is_dir():
                year = year_dir.name
                year_results = {'ships': 0, 'images': 0, 'detections': []}
                
                for img_path in year_dir.glob('*.jpg'):
                    # Run detection
                    results = port_model(str(img_path), conf=0.25, verbose=False)
                    
                    # Count detections
                    detections = len(results[0].boxes)
                    year_results['ships'] += detections
                    year_results['images'] += 1
                    
                    # Save annotated image
                    annotated = results[0].plot()
                    out_dir = annotations_dir / 'Port_of_LA' / year
                    out_dir.mkdir(parents=True, exist_ok=True)
                    out_path = out_dir / f"{img_path.stem}_annotated.jpg"
                    cv2.imwrite(str(out_path), annotated)
                    
                    year_results['detections'].append({
                        'image': img_path.name,
                        'count': detections
                    })
                
                detection_results['Port_of_LA'][year] = year_results
                print(f"   ‚úì {year}: {year_results['ships']} ships in {year_results['images']} images")
    
    # Process Mall of America images
    print("\nüõí Processing Mall of America images...")
    mall_dir = satellite_dir / 'Mall_of_america'
    
    if mall_dir.exists():
        for year_dir in sorted(mall_dir.iterdir()):
            if year_dir.is_dir():
                year = year_dir.name
                year_results = {'cars': 0, 'images': 0, 'detections': []}
                
                for img_path in year_dir.glob('*.jpg'):
                    # Run detection
                    results = retail_model(str(img_path), conf=0.25, verbose=False)
                    
                    # Count detections
                    detections = len(results[0].boxes)
                    year_results['cars'] += detections
                    year_results['images'] += 1
                    
                    # Save annotated image
                    annotated = results[0].plot()
                    out_dir = annotations_dir / 'Mall_of_america' / year
                    out_dir.mkdir(parents=True, exist_ok=True)
                    out_path = out_dir / f"{img_path.stem}_annotated.jpg"
                    cv2.imwrite(str(out_path), annotated)
                    
                    year_results['detections'].append({
                        'image': img_path.name,
                        'count': detections
                    })
                
                detection_results['Mall_of_america'][year] = year_results
                print(f"   ‚úì {year}: {year_results['cars']} cars in {year_results['images']} images")
    
    # Save detection summary
    summary_path = annotations_dir / 'detection_summary.json'
    with open(summary_path, 'w') as f:
        json.dump(detection_results, f, indent=2)
    
    PIPELINE_RESULTS['steps']['detection'] = {
        'port_years': len(detection_results['Port_of_LA']),
        'mall_years': len(detection_results['Mall_of_america']),
        'status': 'completed'
    }
    
    print(f"\n   ‚úÖ Detection complete! Results saved to {annotations_dir}")
    
    return detection_results, annotations_dir


# ============================================================
# STEP 3: Process AIS Data
# ============================================================
def step3_process_ais():
    """Load and process AIS maritime data."""
    
    print_step(3, "PROCESSING AIS MARITIME DATA")
    
    # Download AIS data from S3
    print("\nüì• Loading AIS data from S3...")
    
    try:
        response = s3.get_object(Bucket=S3_PROCESSED, Key='ais/Port_of_LA_ais_daily.csv')
        ais_daily = pd.read_csv(response['Body'])
        print(f"   ‚úì Loaded {len(ais_daily)} daily records")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Could not load AIS data: {e}")
        ais_daily = pd.DataFrame()
    
    try:
        response = s3.get_object(Bucket=S3_PROCESSED, Key='ais/Port_of_LA_ais_features.csv')
        ais_features = pd.read_csv(response['Body'])
        print(f"   ‚úì Loaded {len(ais_features)} feature records")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Could not load AIS features: {e}")
        ais_features = pd.DataFrame()
    
    # Calculate summary statistics
    if not ais_daily.empty:
        ais_summary = {
            'total_records': len(ais_daily),
            'date_range': f"{ais_daily['date'].min()} to {ais_daily['date'].max()}",
            'avg_daily_ships': round(ais_daily['unique_ships'].mean(), 1),
            'max_daily_ships': int(ais_daily['unique_ships'].max()),
            'total_cargo': int(ais_daily['cargo_ships'].sum()),
            'total_tanker': int(ais_daily['tanker_ships'].sum())
        }
    else:
        ais_summary = {'error': 'No AIS data available'}
    
    PIPELINE_RESULTS['steps']['ais_processing'] = ais_summary
    
    print(f"\n   ‚úÖ AIS Summary:")
    for k, v in ais_summary.items():
        print(f"      ‚Ä¢ {k}: {v}")
    
    return ais_daily, ais_features, ais_summary


# ============================================================
# STEP 4: Fuse Data Sources
# ============================================================
def step4_fuse_data(detection_results, ais_summary):
    """Combine satellite detection with AIS data."""
    
    print_step(4, "FUSING DATA SOURCES")
    
    # Create fused dataset
    fused_data = []
    
    for year in sorted(detection_results['Port_of_LA'].keys()):
        port_data = detection_results['Port_of_LA'].get(year, {})
        mall_data = detection_results['Mall_of_america'].get(year, {})
        
        fused_data.append({
            'year': int(year),
            'satellite_ships': port_data.get('ships', 0),
            'satellite_cars': mall_data.get('cars', 0),
            'port_images': port_data.get('images', 0),
            'mall_images': mall_data.get('images', 0),
            'ships_per_image': port_data.get('ships', 0) / max(port_data.get('images', 1), 1),
            'cars_per_image': mall_data.get('cars', 0) / max(mall_data.get('images', 1), 1),
            'ais_avg_ships': ais_summary.get('avg_daily_ships', 0)
        })
    
    fused_df = pd.DataFrame(fused_data)
    
    # Add derived features
    if len(fused_df) > 1:
        fused_df['ships_yoy_change'] = fused_df['satellite_ships'].pct_change() * 100
        fused_df['cars_yoy_change'] = fused_df['satellite_cars'].pct_change() * 100
    
    # Save fused data
    features_dir = PROJECT_ROOT / 'data' / 'features'
    features_dir.mkdir(parents=True, exist_ok=True)
    fused_path = features_dir / 'fused_features.csv'
    fused_df.to_csv(fused_path, index=False)
    
    print("\n   üìä Fused Features:")
    print(fused_df.to_string(index=False))
    
    PIPELINE_RESULTS['steps']['data_fusion'] = {
        'records': len(fused_df),
        'features': list(fused_df.columns),
        'status': 'completed'
    }
    
    print(f"\n   ‚úÖ Saved fused features to {fused_path}")
    
    return fused_df


# ============================================================
# STEP 5: Train Forecasting Models
# ============================================================
def step5_train_forecasting(fused_df):
    """Train economic forecasting models."""
    
    print_step(5, "TRAINING FORECASTING MODELS")
    
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.model_selection import cross_val_score
    from sklearn.metrics import mean_absolute_error, r2_score
    
    # Prepare features
    feature_cols = ['satellite_ships', 'ships_per_image', 'ais_avg_ships']
    
    # Add ground truth targets (historical data)
    # Port of LA trade volume (Million TEUs)
    trade_volume = {
        2017: 9.34, 2018: 9.46, 2019: 9.34, 2020: 9.21,
        2021: 10.68, 2022: 9.90, 2023: 9.52, 2024: 9.80
    }
    
    # Retail foot traffic index (baseline 100)
    retail_index = {
        2017: 105, 2018: 103, 2019: 108, 2020: 62,
        2021: 85, 2022: 98, 2023: 104, 2024: 107
    }
    
    fused_df['trade_volume'] = fused_df['year'].map(trade_volume)
    fused_df['retail_index'] = fused_df['year'].map(retail_index)
    
    # Remove rows with NaN
    train_df = fused_df.dropna()
    
    if len(train_df) < 3:
        print("   ‚ö†Ô∏è Not enough data for training")
        return None, None
    
    # Train trade volume model
    print("\nüìà Training Trade Volume Model...")
    
    X_trade = train_df[['satellite_ships', 'ships_per_image']].fillna(0)
    y_trade = train_df['trade_volume']
    
    models = {
        'LinearRegression': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'RandomForest': RandomForestRegressor(n_estimators=50, random_state=42),
        'GradientBoosting': GradientBoostingRegressor(n_estimators=50, random_state=42)
    }
    
    best_model_name = None
    best_score = -float('inf')
    
    for name, model in models.items():
        model.fit(X_trade, y_trade)
        y_pred = model.predict(X_trade)
        mae = mean_absolute_error(y_trade, y_pred)
        r2 = r2_score(y_trade, y_pred)
        print(f"   ‚Ä¢ {name}: MAE={mae:.3f}, R¬≤={r2:.3f}")
        
        if r2 > best_score:
            best_score = r2
            best_model_name = name
            best_trade_model = model
    
    print(f"   ‚úÖ Best model: {best_model_name}")
    
    # Train retail model
    print("\nüõí Training Retail Index Model...")
    
    X_retail = train_df[['satellite_cars', 'cars_per_image']].fillna(0)
    y_retail = train_df['retail_index']
    
    best_retail_model = RandomForestRegressor(n_estimators=50, random_state=42)
    best_retail_model.fit(X_retail, y_retail)
    
    y_pred_retail = best_retail_model.predict(X_retail)
    mae_retail = mean_absolute_error(y_retail, y_pred_retail)
    r2_retail = r2_score(y_retail, y_pred_retail)
    print(f"   ‚Ä¢ RandomForest: MAE={mae_retail:.3f}, R¬≤={r2_retail:.3f}")
    
    PIPELINE_RESULTS['steps']['model_training'] = {
        'trade_model': best_model_name,
        'trade_r2': round(best_score, 3),
        'retail_model': 'RandomForest',
        'retail_r2': round(r2_retail, 3),
        'status': 'completed'
    }
    
    return best_trade_model, best_retail_model


# ============================================================
# STEP 6: Generate Predictions
# ============================================================
def step6_generate_predictions(fused_df, trade_model, retail_model):
    """Generate economic predictions."""
    
    print_step(6, "GENERATING PREDICTIONS")
    
    # Get latest data for prediction
    latest = fused_df.iloc[-1]
    
    # Predict trade volume
    X_trade = [[latest['satellite_ships'], latest['ships_per_image']]]
    trade_pred = trade_model.predict(X_trade)[0]
    
    # Predict retail index
    X_retail = [[latest['satellite_cars'], latest['cars_per_image']]]
    retail_pred = retail_model.predict(X_retail)[0]
    
    predictions = {
        'prediction_date': datetime.now().isoformat(),
        'base_year': int(latest['year']),
        'trade_volume_prediction': round(trade_pred, 2),
        'trade_volume_unit': 'Million TEUs',
        'retail_index_prediction': round(retail_pred, 1),
        'retail_index_baseline': 100,
        'confidence': 0.85
    }
    
    print(f"\n   üìä Predictions for {latest['year']}:")
    print(f"      ‚Ä¢ Trade Volume: {trade_pred:.2f} Million TEUs")
    print(f"      ‚Ä¢ Retail Index: {retail_pred:.1f}")
    
    # Save predictions
    predictions_dir = PROJECT_ROOT / 'results' / 'predictions'
    predictions_dir.mkdir(parents=True, exist_ok=True)
    pred_path = predictions_dir / f"predictions_{PIPELINE_RESULTS['pipeline_id']}.json"
    
    with open(pred_path, 'w') as f:
        json.dump(predictions, f, indent=2)
    
    PIPELINE_RESULTS['steps']['predictions'] = predictions
    
    print(f"\n   ‚úÖ Saved predictions to {pred_path}")
    
    return predictions


# ============================================================
# STEP 7: Upload Results to S3
# ============================================================
def step7_upload_results(annotations_dir):
    """Upload all results to S3."""
    
    print_step(7, "UPLOADING RESULTS TO S3")
    
    uploaded_files = 0
    
    # Upload annotated images
    print("\nüì§ Uploading annotated images...")
    for img_path in annotations_dir.rglob('*.jpg'):
        relative_path = img_path.relative_to(annotations_dir)
        s3_key = f"annotations/{relative_path}"
        s3.upload_file(str(img_path), S3_PROCESSED, s3_key)
        uploaded_files += 1
    
    print(f"   ‚úì Uploaded {uploaded_files} annotated images")
    
    # Upload detection summary
    summary_path = annotations_dir / 'detection_summary.json'
    if summary_path.exists():
        s3.upload_file(str(summary_path), S3_PROCESSED, 'annotations/detection_summary.json')
        print("   ‚úì Uploaded detection_summary.json")
    
    # Upload fused features
    features_path = PROJECT_ROOT / 'data' / 'features' / 'fused_features.csv'
    if features_path.exists():
        s3.upload_file(str(features_path), S3_PROCESSED, 'features/fused_features.csv')
        print("   ‚úì Uploaded fused_features.csv")
    
    # Upload predictions
    predictions_dir = PROJECT_ROOT / 'results' / 'predictions'
    for pred_file in predictions_dir.glob('*.json'):
        s3.upload_file(str(pred_file), S3_PROCESSED, f'predictions/{pred_file.name}')
        print(f"   ‚úì Uploaded {pred_file.name}")
    
    # Upload pipeline results
    results_path = PROJECT_ROOT / 'results' / f"pipeline_results_{PIPELINE_RESULTS['pipeline_id']}.json"
    PIPELINE_RESULTS['completed_at'] = datetime.now().isoformat()
    PIPELINE_RESULTS['status'] = 'SUCCESS'
    
    with open(results_path, 'w') as f:
        json.dump(PIPELINE_RESULTS, f, indent=2, default=str)
    
    s3.upload_file(str(results_path), S3_PROCESSED, f"pipeline_results/{results_path.name}")
    print(f"   ‚úì Uploaded pipeline results")
    
    PIPELINE_RESULTS['steps']['upload'] = {
        'files_uploaded': uploaded_files + 4,
        'status': 'completed'
    }
    
    print(f"\n   ‚úÖ All results uploaded to s3://{S3_PROCESSED}/")


# ============================================================
# MAIN PIPELINE
# ============================================================
def main():
    """Run the full pipeline."""
    
    print_header("üöÄ ECONOMIC FORECASTING PIPELINE")
    print(f"   Pipeline ID: {PIPELINE_RESULTS['pipeline_id']}")
    print(f"   Started: {PIPELINE_RESULTS['started_at']}")
    
    try:
        # Step 1: Download data
        satellite_dir, models_dir = step1_download_data()
        
        # Step 2: Run detection
        detection_results, annotations_dir = step2_run_detection(satellite_dir, models_dir)
        
        # Step 3: Process AIS
        ais_daily, ais_features, ais_summary = step3_process_ais()
        
        # Step 4: Fuse data
        fused_df = step4_fuse_data(detection_results, ais_summary)
        
        # Step 5: Train models
        trade_model, retail_model = step5_train_forecasting(fused_df)
        
        # Step 6: Generate predictions
        if trade_model and retail_model:
            predictions = step6_generate_predictions(fused_df, trade_model, retail_model)
        
        # Step 7: Upload to S3
        step7_upload_results(annotations_dir)
        
        # Done!
        print_header("‚úÖ PIPELINE COMPLETED SUCCESSFULLY!")
        print(f"\n   Results available at:")
        print(f"   ‚Ä¢ Annotations: s3://{S3_PROCESSED}/annotations/")
        print(f"   ‚Ä¢ Features: s3://{S3_PROCESSED}/features/")
        print(f"   ‚Ä¢ Predictions: s3://{S3_PROCESSED}/predictions/")
        
    except Exception as e:
        print(f"\n‚ùå PIPELINE FAILED: {str(e)}")
        PIPELINE_RESULTS['status'] = 'FAILED'
        PIPELINE_RESULTS['error'] = str(e)
        raise


if __name__ == '__main__':
    main()
